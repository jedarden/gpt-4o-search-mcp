# FastMCP Web Search MCP Server

This project implements an MCP server using FastMCP that exposes a web search tool. The tool accepts a search query, calls the OpenAI API (gpt-4o-search-preview model), and returns structured search results. The server is modular, secure, and ready for containerized deployment.

## Features

- FastMCP-based MCP server
- Web search tool endpoint (`/tools/web_search_tool`) with streaming support
- Secure OpenAI API key management (via environment variable)
- Input validation and robust error handling
- Modular, async code structure
- All completions and streaming handled via the OpenAI Python SDK
- Unit and integration tests with high coverage (≥90%)
- Dockerized for easy deployment
- All dependencies are pinned for security and reproducibility

## Setup

### 1. Clone the repository

```bash
git clone <repo-url>
cd <repo-directory>
```

### 2. Install dependencies

```bash
pip install -r requirements.txt
```

All dependencies are pinned to secure, compatible versions. Note: `fastmcp` is not a public PyPI package; ensure it is available locally or via your organization.

### 3. Configure environment variables

Copy `.env.example` to `.env` and set your OpenAI API key:

```
OPENAI_API_KEY=your-openai-api-key-here
```

### 4. Run the server

```bash
fastmcp run mcp_server/main.py:mcp --transport streamable-http
> **Note:** The `--host` option is not supported in the current FastMCP CLI and should be omitted from the `fastmcp run` command.
> **Direct Python Run (for development/debugging):**  
> If you want to run the server directly (not via the FastMCP CLI), **you must start it from the workspace root using:**  
> 
> ```bash
> python -m mcp_server.main
> ```
> 
> This is required because running `python mcp_server/main.py` sets the module search path to `mcp_server`, not the workspace root, causing import errors like `No module named 'mcp_server'`.  
> Always use `python -m mcp_server.main` from the root directory to ensure correct package imports.
```

The server will be available at `http://localhost:8000`.

> **Note:** The server must be started using the FastMCP CLI entrypoint as shown above. The `--transport streamable-http` option is the recommended transport for modern deployments. SSE is deprecated and should not be used for new deployments.

## Docker Deployment

### Build and run with Docker

```bash
docker build -t fastmcp-server .
docker run --env-file .env -p 8000:8000 fastmcp-server
```

- The container runs as a non-root user for security.
- The `.env` file should be mounted at runtime and not copied into the image.
- The container entrypoint is set to:
  ```
  fastmcp run mcp_server/main.py:mcp --transport streamable-http
  ```
- The `--transport streamable-http` option is required for modern deployments. SSE is deprecated and not recommended.
- For advanced deployment options and troubleshooting, see the [official FastMCP deployment documentation](https://gofastmcp.com/deployment/cli).

## Usage

### Web Search Tool Endpoint

- **Endpoint:** `/tools/web_search_tool`
- **Method:** POST
- **Request Body:** JSON with `query` field (string, required, max 512 chars)

#### Example Request

```json
{
  "query": "What is the Model Context Protocol?"
}
```

#### Example Response (Success)

```json
{
  "success": true,
  "result": "The Model Context Protocol (MCP) is ..."
}
```

#### Example Response (Error)

```json
{
  "success": false,
  "error": "Query cannot be empty."
}
```

### Streaming Support

The web search tool endpoint supports streaming responses using the `streamable-http` transport. To use streaming, set the `Accept: text/event-stream` header in your request. The server will stream partial results as they are generated by the OpenAI API.

#### Example (cURL)

```bash
curl -N -H "Accept: text/event-stream" -H "Content-Type: application/json" \
  -d '{"query": "latest AI research"}' \
  http://localhost:8000/tools/web_search_tool
```

#### Streamed Response Format

Each event contains a JSON object with a partial or final result:

```
data: {"success": true, "result": "Partial or final result text..."}
```

The stream ends when the full result is delivered.

> **Note:** All completions and streaming are handled via the OpenAI Python SDK, ensuring robust and efficient streaming support. The `streamable-http` transport is now the standard; SSE is deprecated.

## Connecting Roo/aiGI or MCP Clients

This server is fully compatible with Roo/aiGI and other Model Context Protocol (MCP) clients. To connect a client agent and access the web search tool, follow these instructions:

### Endpoint URL

- **Base URL:** `http://localhost:8000`
- **MCP Tool Endpoint:** `/tools/web_search_tool`

### Protocol

- **HTTP/1.1** for standard requests
- **Streaming** for real-time responses (set `Accept: text/event-stream` header)

### Authentication

- No authentication is required for MCP clients to connect to this server. The OpenAI API key is managed server-side via environment variables and is never exposed to clients.

### Example: Connecting a Roo/aiGI Agent

To add this MCP server as a tool provider in Roo/aiGI, use the following configuration (replace the URL if running on a different host/port):

```json
{
  "mcp_servers": [
    {
      "name": "fastmcp-web-search",
      "url": "http://localhost:8000"
    }
  ]
}
```

Or, in Python code (using a generic MCP client):

```python
from mcp_client import MCPClient

client = MCPClient("http://localhost:8000")
result = client.use_tool("web_search_tool", {"query": "What is the Model Context Protocol?"})
print(result)
```

### Required Headers

- For standard requests: `Content-Type: application/json`
- For streaming: add `Accept: text/event-stream`


- [Roo/aiGI Documentation](https://github.com/aigi-ai/aigi)
- [Model Context Protocol (MCP) Specification](https://github.com/aigi-ai/model-context-protocol)
- [FastMCP Documentation](https://github.com/aigi-ai/fastmcp)

> **Note:** Ensure the MCP client and this server are on the same network or that the server is accessible from the client environment.

### Roo/aiGI MCP Integration via `streamable-http`

To connect Roo/aiGI (or any aiGI-compatible agent) to this MCP server using the recommended `streamable-http` transport, add the following to your aiGI configuration (e.g., in `aigi.yaml` or your agent's config file):

```yaml
mcp:
  transport: streamable-http
  endpoint: http://localhost:8000/mcp
  # headers:
  #   Authorization: "Bearer <YOUR_TOKEN>"
```

- Set `endpoint` to the MCP server's `/mcp` endpoint (adjust host/port as needed).
- Uncomment and set the `Authorization` header if your deployment requires authentication.
- The `streamable-http` transport is recommended for all modern aiGI and Roo deployments.

#### Best Practices & References

- Always use the `streamable-http` transport for new deployments; SSE is deprecated.
- Ensure the MCP server is accessible from your agent's environment (network/firewall).
- Keep your aiGI and MCP server versions up to date for best compatibility.
- For advanced configuration, see:
  - [Official MCP Documentation](https://github.com/aigi-ai/model-context-protocol)
  - [streamable-http Transport Reference](https://github.com/aigi-ai/model-context-protocol/blob/main/docs/streamable-http.md)
  - [Roo/aiGI Integration Guide](https://github.com/aigi-ai/aigi)

## Testing
## Testing

Run the test suite with:

```bash
pytest
```

To check test coverage:

```bash
pytest --cov=mcp_server --cov-report=term-missing
```

- The enhanced test suite covers valid/invalid queries, error handling, streaming behavior, and security (API key never exposed).
- Coverage is ≥90% for all major code paths, including streaming and error scenarios.

## Security Notes

- The OpenAI API key is loaded from the environment and never exposed to clients or logs.
- Do not commit your real API key to source control.
- The Docker container runs as a non-root user.
- All dependencies are pinned to specific versions to reduce supply chain risk.
- The server validates all input and handles errors gracefully to prevent information leakage.

## License

MIT